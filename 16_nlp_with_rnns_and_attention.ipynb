{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXQ0OkANWSA2"
      },
      "source": [
        "**Chapter 16 – Natural Language Processing with RNNs and Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtP0_-H8WSA3"
      },
      "source": [
        "_This notebook contains all the sample code and solutions to the exercises in chapter 16._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5pv_MpYWSA3"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/ageron/handson-ml3/blob/main/16_nlp_with_rnns_and_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml3/blob/main/16_nlp_with_rnns_and_attention.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFXIv9qNpKzt",
        "tags": []
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IPbJEmZpKzu"
      },
      "source": [
        "This project requires Python 3.7 or above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TFSU3FCOpKzu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XdVVIm7WSA4"
      },
      "source": [
        "**Warning**: the latest TensorFlow versions are based on Keras 3. For previous chapters, it wasn't too hard to update the code to support Keras 3, but unfortunately it's much harder for this chapter: for example, stateful RNNs work very differently, ragged tensors are no longer supported, TensorFlow Hub models are no longer supported, and more. So for this chapter I've had to revert to Keras 2. To do that, I set the `TF_USE_LEGACY_KERAS` environment variable to `\"1\"` and import the `tf_keras` package. This ensures that `tf.keras` points to `tf_keras`, which is Keras 2.*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J9ZbKeRUWSA4"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "if IS_COLAB:\n",
        "    import os\n",
        "    os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "    import tf_keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJtVEqxfpKzw"
      },
      "source": [
        "And TensorFlow ≥ 2.8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0Piq5se2pKzx"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDaDoLQTpKzx"
      },
      "source": [
        "As we did in earlier chapters, let's define the default font sizes to make the figures prettier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8d4TH3NbpKzx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcoUIRsvpKzy"
      },
      "source": [
        "And let's create the `images/nlp` folder (if it doesn't already exist), and define the `save_fig()` function which is used through this notebook to save the figures in high-res for the book:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PQFH5Y9PpKzy"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"nlp\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTsawKlapKzy"
      },
      "source": [
        "This chapter can be very slow without a GPU, so let's make sure there's one, or else issue a warning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ekxzo6pOpKzy"
      },
      "outputs": [],
      "source": [
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
        "              \"accelerator.\")\n",
        "    if \"kaggle_secrets\" in sys.modules:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3b1xOwgWSA6"
      },
      "source": [
        "# Generating Shakespearean Text Using a Character RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f511pwcbWSA6"
      },
      "source": [
        "## Creating the Training Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzUnPEI-WSA6"
      },
      "source": [
        "Let's download the Shakespeare data from Andrej Karpathy's [char-rnn project](https://github.com/karpathy/char-rnn/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ktKDSwi_WSA7",
        "outputId": "8637f500-396b-4561-9390-88604f33bd57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
        "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3mCybM1KWSA7",
        "outputId": "80d45847-1496-4033-c5af-a7551ae1a4b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n"
          ]
        }
      ],
      "source": [
        "# extra code – shows a short text sample\n",
        "print(shakespeare_text[:80])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nnh7zS3LWSA7",
        "outputId": "85cfa005-74ae-41b8-acd4-acd6baac50cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# extra code – shows all 39 distinct characters (after converting to lower case)\n",
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OfYtWrOyWSA7"
      },
      "outputs": [],
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
        "                                                   standardize=\"lower\")\n",
        "text_vec_layer.adapt([shakespeare_text])\n",
        "encoded = text_vec_layer([shakespeare_text])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WhV7GP3AWSA7"
      },
      "outputs": [],
      "source": [
        "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
        "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
        "dataset_size = len(encoded)  # total number of chars = 1,115,394"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zw-ynTIUWSA7",
        "outputId": "c99eac4e-2e82-4380-ea6e-4f556a6319e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "n_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pTfqnCsVWSA7",
        "outputId": "134f7f68-4423-4629-f939-46b8c7851b96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bmdVK792WSA8"
      },
      "outputs": [],
      "source": [
        "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(100_000, seed=seed)\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OHob9X11WSA8",
        "outputId": "bf367c2b-dc16-4ca7-d34f-1fec36fa4a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 4,  5,  2, 23]])>,\n",
              "  <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 5,  2, 23,  3]])>)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# extra code – a simple example using to_dataset()\n",
        "# There's just one sample in this dataset: the input represents \"to b\" and the\n",
        "# output represents \"o be\"\n",
        "list(to_dataset(text_vec_layer([\"To be\"])[0], length=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ex8fZI0MWSA8"
      },
      "outputs": [],
      "source": [
        "length = 100\n",
        "tf.random.set_seed(42)\n",
        "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
        "                       seed=42)\n",
        "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
        "test_set = to_dataset(encoded[1_060_000:], length=length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYp0HyK2WSA8"
      },
      "source": [
        "## Building and Training the Char-RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNNzGSlnWSA8"
      },
      "source": [
        "**Warning**: the following code may one or two hours to run, depending on your GPU. Without a GPU, it may take over 24 hours. If you don't want to wait, just skip the next two code cells and run the code below to download a pretrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk8hCBYAWSA8"
      },
      "source": [
        "**Note**: the `GRU` class will only use cuDNN acceleration (assuming you have a GPU) when using the default values for the following arguments: `activation`, `recurrent_activation`, `recurrent_dropout`, `unroll`, `use_bias` and `reset_after`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DD0NWwp8WSA8",
        "outputId": "fa8f474c-186f-456c-bb06-0406a5d7ac0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "31247/31247 [==============================] - 418s 12ms/step - loss: 1.3978 - accuracy: 0.5721 - val_loss: 1.5926 - val_accuracy: 0.5360\n",
            "Epoch 2/10\n",
            "31247/31247 [==============================] - 376s 11ms/step - loss: 1.2953 - accuracy: 0.5959 - val_loss: 1.5707 - val_accuracy: 0.5438\n",
            "Epoch 3/10\n",
            "22962/31247 [=====================>........] - ETA: 1:51 - loss: 1.2753 - accuracy: 0.6005"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3926721648.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n\u001b[1;32m     10\u001b[0m     \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n\u001b[0;32m---> 11\u001b[0;31m history = model.fit(train_set, validation_data=valid_set, epochs=10,\n\u001b[0m\u001b[1;32m     12\u001b[0m                     callbacks=[model_ckpt])\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1802\u001b[0m                         ):\n\u001b[1;32m   1803\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
        "                    callbacks=[model_ckpt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rTw3f2BsWSA8"
      },
      "outputs": [],
      "source": [
        "shakespeare_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
        "    model\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbPw_sq7WSA8"
      },
      "source": [
        "If you don't want to wait for training to complete, I've pretrained a model for you. The following code will download it. Uncomment the last line if you want to use it instead of the model trained above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QGliqvn4WSA8"
      },
      "outputs": [],
      "source": [
        "# extra code – downloads a pretrained model\n",
        "url = \"https://github.com/ageron/data/raw/main/shakespeare_model.tgz\"\n",
        "path = tf.keras.utils.get_file(\"shakespeare_model.tgz\", url, extract=True)\n",
        "if \"_extracted\" in path:\n",
        "    model_path = Path(path) / \"shakespeare_model\"\n",
        "else:\n",
        "    model_path = Path(path).with_name(\"shakespeare_model\")\n",
        "#shakespeare_model = tf.keras.models.load_model(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "f_m7chlKWSA9",
        "outputId": "00eb3b2c-746b-4a45-fb58-8263f4cd150a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 447ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.str_('-')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
        "y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
        "text_vec_layer.get_vocabulary()[y_pred + 2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67qetJn7WSA9"
      },
      "source": [
        "## Generating Fake Shakespearean Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LdgYUtxOWSA9",
        "outputId": "5bd9949b-a09c-4769-f31d-ee34d3bdc076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 0, 1, 1, 1, 0, 0, 0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
        "tf.random.set_seed(42)\n",
        "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-HbDRGsqWSA9"
      },
      "outputs": [],
      "source": [
        "def next_char(text, temperature=1):\n",
        "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
        "    return text_vec_layer.get_vocabulary()[char_id + 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qTgER62AWSA9"
      },
      "outputs": [],
      "source": [
        "def extend_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tUvVU4MQWSA9"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Y1EdBpl9WSA9",
        "outputId": "d171b187-5024-4ab7-c3fc-b8f1ca0df3bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "To be or not to besfq?d\n",
            "dsidn:;&yoer3\n",
            "jc&lvj,sypxh. b:kbgor wonitj'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(extend_text(\"To be or not to be\", temperature=0.01))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Yzy9Yn1hWSA9",
        "outputId": "c8e9b7d8-8c45-4e08-b57e-976383941fe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "To be or not to beg3cc bp\n",
            "dm;'aet.nyr$bjtx\n",
            "qh,cr:\n",
            "v3-oevb-si?xr&zl3y\n"
          ]
        }
      ],
      "source": [
        "print(extend_text(\"To be or not to be\", temperature=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WzwPhZhEWSA-",
        "outputId": "30260aba-5f65-476e-e9cd-8bc40628d5fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "To be or not to bepevi$m-v lv!?$mz?gmjz :3?ljb'va;!td&hi.ur3l'-j!3eu\n"
          ]
        }
      ],
      "source": [
        "print(extend_text(\"To be or not to be\", temperature=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsVnv5xJWSA-"
      },
      "source": [
        "## Stateful RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HdRdTXVcWSA-"
      },
      "outputs": [],
      "source": [
        "def to_dataset_for_stateful_rnn(sequence, length):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda window: window.batch(length + 1)).batch(1)\n",
        "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
        "\n",
        "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:1_000_000], length)\n",
        "stateful_valid_set = to_dataset_for_stateful_rnn(encoded[1_000_000:1_060_000],\n",
        "                                                 length)\n",
        "stateful_test_set = to_dataset_for_stateful_rnn(encoded[1_060_000:], length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XR7xYmfJWSA-",
        "outputId": "c3d7a2a3-5bc1-4cc8-e472-e7218fd2b790",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[0, 1, 2]], dtype=int32)>,\n",
              "  <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[1, 2, 3]], dtype=int32)>),\n",
              " (<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[3, 4, 5]], dtype=int32)>,\n",
              "  <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[4, 5, 6]], dtype=int32)>),\n",
              " (<tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[6, 7, 8]], dtype=int32)>,\n",
              "  <tf.Tensor: shape=(1, 3), dtype=int32, numpy=array([[7, 8, 9]], dtype=int32)>)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# extra code – simple example using to_dataset_for_stateful_rnn()\n",
        "list(to_dataset_for_stateful_rnn(tf.range(10), 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzjVVOYQWSA-"
      },
      "source": [
        "If you'd like to have more than one window per batch, you can use the `to_batched_dataset_for_stateful_rnn()` function instead of `to_dataset_for_stateful_rnn()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "f403Z39zWSA-",
        "outputId": "f499c859-a467-4741-ac46-97040b176256",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
              "  array([[ 0,  1,  2],\n",
              "         [10, 11, 12]], dtype=int32)>,\n",
              "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
              "  array([[ 1,  2,  3],\n",
              "         [11, 12, 13]], dtype=int32)>),\n",
              " (<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
              "  array([[ 3,  4,  5],\n",
              "         [13, 14, 15]], dtype=int32)>,\n",
              "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
              "  array([[ 4,  5,  6],\n",
              "         [14, 15, 16]], dtype=int32)>),\n",
              " (<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
              "  array([[ 6,  7,  8],\n",
              "         [16, 17, 18]], dtype=int32)>,\n",
              "  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
              "  array([[ 7,  8,  9],\n",
              "         [17, 18, 19]], dtype=int32)>)]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# extra code – shows one way to prepare a batched dataset for a stateful RNN\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def to_non_overlapping_windows(sequence, length):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
        "    return ds.flat_map(lambda window: window.batch(length + 1))\n",
        "\n",
        "def to_batched_dataset_for_stateful_rnn(sequence, length, batch_size=32):\n",
        "    parts = np.array_split(sequence, batch_size)\n",
        "    datasets = tuple(to_non_overlapping_windows(part, length) for part in parts)\n",
        "    ds = tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))\n",
        "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
        "\n",
        "list(to_batched_dataset_for_stateful_rnn(tf.range(20), length=3, batch_size=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "eJr-eQNpWSA-"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16,\n",
        "                              batch_input_shape=[1, None]),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True, stateful=True),\n",
        "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lRuK3PvKWSA-"
      },
      "outputs": [],
      "source": [
        "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs):\n",
        "        self.model.reset_states()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GNOG4WSzWSA-"
      },
      "outputs": [],
      "source": [
        "# extra code – use a different directory to save the checkpoints\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"my_stateful_shakespeare_model\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQYqOoiAWSA-"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly an hour if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "637NUtxLWSA-",
        "outputId": "3a39db00-1d15-4eba-ebc0-4d665bc87866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "9999/9999 [==============================] - 90s 9ms/step - loss: 1.8693 - accuracy: 0.4503 - val_loss: 1.7153 - val_accuracy: 0.4857\n",
            "Epoch 2/10\n",
            "9999/9999 [==============================] - 82s 8ms/step - loss: 1.5673 - accuracy: 0.5266 - val_loss: 1.6262 - val_accuracy: 0.5134\n",
            "Epoch 3/10\n",
            "2257/9999 [=====>........................] - ETA: 1:05 - loss: 1.5004 - accuracy: 0.5446"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2733108475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n\u001b[1;32m      2\u001b[0m               metrics=[\"accuracy\"])\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n\u001b[0m\u001b[1;32m      4\u001b[0m                     epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \"\"\"\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mprovided\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m   \"\"\"\n\u001b[0;32m--> 628\u001b[0;31m   return nest_util.map_structure(\n\u001b[0m\u001b[1;32m    629\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m   \"\"\"\n\u001b[1;32m   1064\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_core_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_data_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1103\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1104\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \"\"\"\n\u001b[1;32m    418\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayLike\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(stateful_train_set, validation_data=stateful_valid_set,\n",
        "                    epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53GmODb0WSA-"
      },
      "source": [
        "**Extra Material: converting the stateful RNN to a stateless RNN and using it**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZU6fRhSWSA_"
      },
      "source": [
        "To use the model with different batch sizes, we need to create a stateless copy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8slqih2BWSA_"
      },
      "outputs": [],
      "source": [
        "stateless_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
        "    tf.keras.layers.GRU(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXS_WOjGWSA_"
      },
      "source": [
        "To set the weights, we first need to build the model (so the weights get created):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UNgX5agWSA_"
      },
      "outputs": [],
      "source": [
        "stateless_model.build(tf.TensorShape([None, None]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2HDC5Q4WSA_"
      },
      "outputs": [],
      "source": [
        "stateless_model.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiEw4Z1XWSA_"
      },
      "outputs": [],
      "source": [
        "shakespeare_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
        "    stateless_model\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsi-ypRxWSA_"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "print(extend_text(\"to be or not to be\", temperature=0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y1K9kt0WSA_"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqhW1FWiWSA_"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
        "    name=\"imdb_reviews\",\n",
        "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
        "    as_supervised=True\n",
        ")\n",
        "tf.random.set_seed(42)\n",
        "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
        "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
        "test_set = raw_test_set.batch(32).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQCY5Mv-WSA_"
      },
      "outputs": [],
      "source": [
        "for review, label in raw_train_set.take(4):\n",
        "    print(review.numpy().decode(\"utf-8\")[:200], \"...\")\n",
        "    print(\"Label:\", label.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfqoSqg_WSA_"
      },
      "outputs": [],
      "source": [
        "vocab_size = 1000\n",
        "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
        "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajZekvhDWSA_"
      },
      "source": [
        "**Warning**: the following cell will take a few minutes to run and the model will probably not learn anything because we didn't mask the padding tokens (that's the point of the next section)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8deROW7WSBA"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xCXRA77WSBA"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqHn0zZVWSBA"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly 30 minutes if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0k6Hz6yWSBA"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H9zVV8iWSBA"
      },
      "source": [
        "Or using manual masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hU8apn7WSBA"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on the CPU\n",
        "inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "token_ids = text_vec_layer(inputs)\n",
        "mask = tf.math.not_equal(token_ids, 0)\n",
        "Z = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\n",
        "Z = tf.keras.layers.GRU(128, dropout=0.2)(Z, mask=mask)\n",
        "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(Z)\n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlrOQgQIWSBA"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly 30 minutes if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_5PR7RnWSBA"
      },
      "outputs": [],
      "source": [
        "# extra code – compiles and trains the model, as usual\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnebGCt0WSBA"
      },
      "source": [
        "**Extra material: using ragged tensors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T92UIIZOWSBA"
      },
      "outputs": [],
      "source": [
        "text_vec_layer_ragged = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size, ragged=True)\n",
        "text_vec_layer_ragged.adapt(train_set.map(lambda reviews, labels: reviews))\n",
        "text_vec_layer_ragged([\"Great movie!\", \"This is DiCaprio's best role.\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biGy7QBOWSBA"
      },
      "outputs": [],
      "source": [
        "text_vec_layer([\"Great movie!\", \"This is DiCaprio's best role.\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QrQY8VIWSBA"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly 30 minutes if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4OhDDrbWSBA"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer_ragged,\n",
        "    tf.keras.layers.Embedding(vocab_size, embed_size),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=valid_set, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGPZvT_sWSBB"
      },
      "source": [
        "## Reusing Pretrained Embeddings and Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXfSzyFnWSBB"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly an hour if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG14IeIKWSBB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "os.environ[\"TFHUB_CACHE_DIR\"] = \"my_tfhub_cache\"\n",
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                   trainable=True, dtype=tf.string, input_shape=[]),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_set, validation_data=valid_set, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk5--CJBWSBB"
      },
      "source": [
        "# An Encoder–Decoder Network for Neural Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFw-X70EWSBB"
      },
      "outputs": [],
      "source": [
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
        "                               extract=True)\n",
        "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r0Py5FkWSBB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX19beyhWSBB"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    print(sentences_en[i], \"=>\", sentences_es[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1u6U4mvWSBB"
      },
      "outputs": [],
      "source": [
        "vocab_size = 1000\n",
        "max_length = 50\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length=max_length)\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4Ih0DC2WSBB"
      },
      "outputs": [],
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhcCbmlfWSBB"
      },
      "outputs": [],
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR8Si0ARWSBB"
      },
      "outputs": [],
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fa1fVTYWSBB"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbaYk-Y1WSBB"
      },
      "outputs": [],
      "source": [
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqkz6A5TWSBB"
      },
      "outputs": [],
      "source": [
        "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pleyTaa3WSBC"
      },
      "outputs": [],
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zvIlIdgWSBC"
      },
      "outputs": [],
      "source": [
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li4cmcKVWSBC"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4ZXzvhsWSBC"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI8ihVgqWSBC"
      },
      "outputs": [],
      "source": [
        "def translate(sentence_en):\n",
        "    translation = \"\"\n",
        "    for word_idx in range(max_length):\n",
        "        X = np.array([sentence_en])  # encoder input\n",
        "        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
        "        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "        if predicted_word == \"endofseq\":\n",
        "            break\n",
        "        translation += \" \" + predicted_word\n",
        "    return translation.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_e170NNWSBC"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn1KJErcWSBC"
      },
      "source": [
        "Nice! However, the model struggles with longer sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azx02JDXWSBC"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5ka-XQVWSBC"
      },
      "source": [
        "## Bidirectional RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L-JdhUaWSBC"
      },
      "source": [
        "To create a bidirectional recurrent layer, just wrap a regular recurrent layer in a `Bidirectional` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMI4o_cqWSBC"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_state=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM1AO6HFWSBC"
      },
      "outputs": [],
      "source": [
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
        "                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8VT2dfgWSBC"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwUFhyucWSBC"
      },
      "outputs": [],
      "source": [
        "# extra code — completes the model and trains it\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(decoder_outputs)\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjOi_nKuWSBC"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciifBxuuWSBD"
      },
      "source": [
        "## Beam Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eYadIBNWSBD"
      },
      "source": [
        "This is a very basic implementation of beam search. I tried to make it readable and understandable, but it's definitely not optimized for speed! The function first uses the model to find the top _k_ words to start the translations (where _k_ is the beam width). For each of the top _k_ translations, it evaluates the conditional probabilities of all possible words it could add to that translation. These extended translations and their probabilities are added to the list of candidates. Once we've gone through all top _k_ translations and all words that could complete them, we keep only the top _k_ candidates with the highest probability, and we iterate over and over until they all finish with an EOS token. The top translation is then returned (after removing its EOS token).\n",
        "\n",
        "* Note: If p(S) is the probability of sentence S, and p(W|S) is the conditional probability of the word W given that the translation starts with S, then the probability of the sentence S' = concat(S, W) is p(S') = p(S) * p(W|S). As we add more words, the probability gets smaller and smaller. To avoid the risk of it getting too small, which could cause floating point precision errors, the function keeps track of log probabilities instead of probabilities: recall that log(a\\*b) = log(a) + log(b), therefore log(p(S')) = log(p(S)) + log(p(W|S))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW3uYX2fWSBD"
      },
      "outputs": [],
      "source": [
        "# extra code – a basic implementation of beam search\n",
        "\n",
        "def beam_search(sentence_en, beam_width, verbose=False):\n",
        "    X = np.array([sentence_en])  # encoder input\n",
        "    X_dec = np.array([\"startofseq\"])  # decoder input\n",
        "    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n",
        "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
        "    top_translations = [  # list of best (log_proba, translation)\n",
        "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
        "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
        "    ]\n",
        "\n",
        "    # extra code – displays the top first words in verbose mode\n",
        "    if verbose:\n",
        "        print(\"Top first words:\", top_translations)\n",
        "\n",
        "    for idx in range(1, max_length):\n",
        "        candidates = []\n",
        "        for log_proba, translation in top_translations:\n",
        "            if translation.endswith(\"endofseq\"):\n",
        "                candidates.append((log_proba, translation))\n",
        "                continue  # translation is finished, so don't try to extend it\n",
        "            X = np.array([sentence_en])  # encoder input\n",
        "            X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
        "            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n",
        "            for word_id, word_proba in enumerate(y_proba):\n",
        "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
        "                candidates.append((log_proba + np.log(word_proba),\n",
        "                                   f\"{translation} {word}\"))\n",
        "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
        "\n",
        "        # extra code – displays the top translation so far in verbose mode\n",
        "        if verbose:\n",
        "            print(\"Top translations so far:\", top_translations)\n",
        "\n",
        "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
        "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqaH1RS0WSBD"
      },
      "outputs": [],
      "source": [
        "# extra code – shows how the model making an error\n",
        "sentence_en = \"I love cats and dogs\"\n",
        "translate(sentence_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thHAteaFWSBD"
      },
      "outputs": [],
      "source": [
        "# extra code – shows how beam search can help\n",
        "beam_search(sentence_en, beam_width=3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbQbfBmTWSBD"
      },
      "source": [
        "The correct translation is in the top 3 sentences found by beam search, but it's not the first. Since we're using a small vocabulary, the \\[UNK] token is quite frequent, so you may want to penalize it (e.g., divide its probability by 2 in the beam search function): this will discourage beam search from using it too much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZssAnGDWSBD"
      },
      "source": [
        "# Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_2m5O57WSBD"
      },
      "source": [
        "We need to feed all the encoder's outputs to the `Attention` layer, so we must add `return_sequences=True` to the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPd731gdWSBD"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqBLYislWSBD"
      },
      "outputs": [],
      "source": [
        "# extra code – this part of the model is exactly the same as earlier\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
        "                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlhGnTpZWSBD"
      },
      "source": [
        "And finally, let's add the `Attention` layer and the output layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xO1Z5cPWSBD"
      },
      "outputs": [],
      "source": [
        "attention_layer = tf.keras.layers.Attention()\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "Y_proba = output_layer(attention_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1StJi0AQWSBD"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly a couple hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irjbeH1FWSBD"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMIyqB8tWSBD"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3VtxsGuWSBE"
      },
      "outputs": [],
      "source": [
        "beam_search(\"I like soccer and also going to the beach\", beam_width=3,\n",
        "            verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2l1w8MUWSBE"
      },
      "source": [
        "## Attention Is All You Need: The Transformer Architecture\n",
        "### Positional encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhNTA6ONWSBE"
      },
      "outputs": [],
      "source": [
        "max_length = 50  # max length in the whole training set\n",
        "embed_size = 128\n",
        "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
        "pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
        "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
        "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_enc))\n",
        "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
        "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqPK1gZIWSBE"
      },
      "source": [
        "Alternatively, we can use fixed, non-trainable positional encodings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRYbEcUNWSBE"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
        "        p, i = np.meshgrid(np.arange(max_length),\n",
        "                           2 * np.arange(embed_size // 2))\n",
        "        pos_emb = np.empty((1, max_length, embed_size))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
        "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_max_length = tf.shape(inputs)[1]\n",
        "        return inputs + self.pos_encodings[:, :batch_max_length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8FwBw51WSBE"
      },
      "outputs": [],
      "source": [
        "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
        "encoder_in = pos_embed_layer(encoder_embeddings)\n",
        "decoder_in = pos_embed_layer(decoder_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc__HeE5WSBE"
      },
      "outputs": [],
      "source": [
        "# extra code – this cells generates and saves Figure 16–9\n",
        "figure_max_length = 201\n",
        "figure_embed_size = 512\n",
        "pos_emb = PositionalEncoding(figure_max_length, figure_embed_size)\n",
        "zeros = np.zeros((1, figure_max_length, figure_embed_size), np.float32)\n",
        "P = pos_emb(zeros)[0].numpy()\n",
        "i1, i2, crop_i = 100, 101, 150\n",
        "p1, p2, p3 = 22, 60, 35\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\n",
        "ax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\n",
        "ax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\n",
        "ax1.plot(p3, P[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\n",
        "ax1.plot(P[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\n",
        "ax1.plot(P[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\n",
        "ax1.plot([p1, p2], [P[p1, i1], P[p2, i1]], \"bo\")\n",
        "ax1.plot([p1, p2], [P[p1, i2], P[p2, i2]], \"ro\")\n",
        "ax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\n",
        "ax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.hlines(0, 0, figure_max_length - 1, color=\"k\", linewidth=1, alpha=0.3)\n",
        "ax1.axis([0, figure_max_length - 1, -1, 1])\n",
        "ax2.imshow(P.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\n",
        "ax2.hlines(i1, 0, figure_max_length - 1, color=\"b\", linewidth=3)\n",
        "cheat = 2  # need to raise the red line a bit, or else it hides the blue one\n",
        "ax2.hlines(i2+cheat, 0, figure_max_length - 1, color=\"r\", linewidth=3)\n",
        "ax2.plot([p1, p1], [0, crop_i], \"k--\")\n",
        "ax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\n",
        "ax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\n",
        "ax2.plot([p1, p2], [i1, i1], \"bo\")\n",
        "ax2.axis([0, figure_max_length - 1, 0, crop_i])\n",
        "ax2.set_xlabel(\"$p$\", fontsize=16)\n",
        "ax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\n",
        "save_fig(\"positional_embedding_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lruVQLw0WSBE"
      },
      "source": [
        "### Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkNF4gCjWSBE"
      },
      "outputs": [],
      "source": [
        "N = 2  # instead of 6\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "n_units = 128  # for the first Dense layer in each Feed Forward block\n",
        "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
        "Z = encoder_in\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NToAqsArWSBE"
      },
      "outputs": [],
      "source": [
        "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
        "causal_mask = tf.linalg.band_part(  # creates a lower triangular matrix\n",
        "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LENppG3qWSBE"
      },
      "outputs": [],
      "source": [
        "encoder_outputs = Z  # let's save the encoder's final outputs\n",
        "Z = decoder_in  # the decoder starts with its own inputs\n",
        "for _ in range(N):\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
        "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
        "    skip = Z\n",
        "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
        "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
        "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsY9Wnn6WSBE"
      },
      "source": [
        "**Warning**: the following cell will take a while to run (possibly 2 or 3 hours if you are not using a GPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DI_V8SMWSBE"
      },
      "outputs": [],
      "source": [
        "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REsryKOvWSBE"
      },
      "outputs": [],
      "source": [
        "translate(\"I like soccer and also going to the beach\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rnsz3gKWSBF"
      },
      "source": [
        "# HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giIea41sWSBF"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")  # many other tasks are available\n",
        "result = classifier(\"The actors were very convincing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gssDU21WSBF"
      },
      "source": [
        "Models can be very biased. For example, it may like or dislike some countries depending on the data it was trained on, and how it is used, so use it with care:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wcYpv1VWSBF"
      },
      "outputs": [],
      "source": [
        "classifier([\"I am from India.\", \"I am from Iraq.\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvLmKRo5WSBF"
      },
      "outputs": [],
      "source": [
        "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
        "classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
        "classifier_mnli(\"She loves me. [SEP] She loves me not.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eev9nQshWSBF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn4RLDmGWSBF"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n",
        "                       \"Joe lived for a very long time. [SEP] Joe is old.\"],\n",
        "                      padding=True, return_tensors=\"tf\")\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf0WB9ZzWSBF"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer([(\"I like soccer.\", \"We all love soccer!\"),\n",
        "                       (\"Joe lived for a very long time.\", \"Joe is old.\")],\n",
        "                      padding=True, return_tensors=\"tf\")\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TNs9sBRWSBF"
      },
      "outputs": [],
      "source": [
        "outputs = model(token_ids)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9siPIrtWSBF"
      },
      "outputs": [],
      "source": [
        "Y_probas = tf.keras.activations.softmax(outputs.logits)\n",
        "Y_probas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w20jRXJkWSBF"
      },
      "outputs": [],
      "source": [
        "Y_pred = tf.argmax(Y_probas, axis=1)\n",
        "Y_pred  # 0 = contradiction, 1 = entailment, 2 = neutral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk-AYAFiWSBF"
      },
      "outputs": [],
      "source": [
        "sentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\n",
        "X_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\n",
        "y_train = tf.constant([0, 2])  # contradiction, neutral\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG6T1YlVWSBG"
      },
      "source": [
        "# Exercise solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkqqXZPmWSBG"
      },
      "source": [
        "## 1. to 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4LvAPBAWSBG"
      },
      "source": [
        "1. Stateless RNNs can only capture patterns whose length is less than, or equal to, the size of the windows the RNN is trained on. Conversely, stateful RNNs can capture longer-term patterns. However, implementing a stateful RNN is much harder⁠—especially preparing the dataset properly. Moreover, stateful RNNs do not always work better, in part because consecutive batches are not independent and identically distributed (IID). Gradient Descent is not fond of non-IID datasets.\n",
        "2. In general, if you translate a sentence one word at a time, the result will be terrible. For example, the French sentence \"Je vous en prie\" means \"You are welcome,\" but if you translate it one word at a time, you get \"I you in pray.\" Huh? It is much better to read the whole sentence first and then translate it. A plain sequence-to-sequence RNN would start translating a sentence immediately after reading the first word, while an Encoder–Decoder RNN will first read the whole sentence and then translate it. That said, one could imagine a plain sequence-to-sequence RNN that would output silence whenever it is unsure about what to say next (just like human translators do when they must translate a live broadcast).\n",
        "3. Variable-length input sequences can be handled by padding the shorter sequences so that all sequences in a batch have the same length, and using masking to ensure the RNN ignores the padding token. For better performance, you may also want to create batches containing sequences of similar sizes. Ragged tensors can hold sequences of variable lengths, and Keras now supports them, which simplifies handling variable-length input sequences (at the time of this writing, it still does not handle ragged tensors as targets on the GPU, though). Regarding variable-length output sequences, if the length of the output sequence is known in advance (e.g., if you know that it is the same as the input sequence), then you just need to configure the loss function so that it ignores tokens that come after the end of the sequence. Similarly, the code that will use the model should ignore tokens beyond the end of the sequence. But generally the length of the output sequence is not known ahead of time, so the solution is to train the model so that it outputs an end-of-sequence token at the end of each sequence.\n",
        "4. Beam search is a technique used to improve the performance of a trained Encoder–Decoder model, for example in a neural machine translation system. The algorithm keeps track of a short list of the _k_ most promising output sentences (say, the top three), and at each decoder step it tries to extend them by one word; then it keeps only the _k_ most likely sentences. The parameter _k_ is called the _beam width_: the larger it is, the more CPU and RAM will be used, but also the more accurate the system will be. Instead of greedily choosing the most likely next word at each step to extend a single sentence, this technique allows the system to explore several promising sentences simultaneously. Moreover, this technique lends itself well to parallelization. You can implement beam search by writing a custom memory cell. Alternatively, TensorFlow Addons's seq2seq API provides an implementation.\n",
        "5. An attention mechanism is a technique initially used in Encoder–Decoder models to give the decoder more direct access to the input sequence, allowing it to deal with longer input sequences. At each decoder time step, the current decoder's state and the full output of the encoder are processed by an alignment model that outputs an alignment score for each input time step. This score indicates which part of the input is most relevant to the current decoder time step. The weighted sum of the encoder output (weighted by their alignment score) is then fed to the decoder, which produces the next decoder state and the output for this time step. The main benefit of using an attention mechanism is the fact that the Encoder–Decoder model can successfully process longer input sequences. Another benefit is that the alignment scores make the model easier to debug and interpret: for example, if the model makes a mistake, you can look at which part of the input it was paying attention to, and this can help diagnose the issue. An attention mechanism is also at the core of the Transformer architecture, in the Multi-Head Attention layers. See the next answer.\n",
        "6. The most important layer in the Transformer architecture is the Multi-Head Attention layer (the original Transformer architecture contains 18 of them, including 6 Masked Multi-Head Attention layers). It is at the core of language models such as BERT and GPT-2. Its purpose is to allow the model to identify which words are most aligned with each other, and then improve each word's representation using these contextual clues.\n",
        "7. Sampled softmax is used when training a classification model when there are many classes (e.g., thousands). It computes an approximation of the cross-entropy loss based on the logit predicted by the model for the correct class, and the predicted logits for a sample of incorrect words. This speeds up training considerably compared to computing the softmax over all logits and then estimating the cross-entropy loss. After training, the model can be used normally, using the regular softmax function to compute all the class probabilities based on all the logits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mceZC-xTWSBG"
      },
      "source": [
        "## 8.\n",
        "_Exercise:_ Embedded Reber grammars _were used by Hochreiter and Schmidhuber in [their paper](https://homl.info/93) about LSTMs. They are artificial grammars that produce strings such as \"BPBTSXXVPSEPE.\" Check out Jenny Orr's [nice introduction](https://homl.info/108) to this topic. Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr's page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don't._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcgE0ZXyWSBG"
      },
      "source": [
        "First we need to build a function that generates strings based on a grammar. The grammar will be represented as a list of possible transitions for each state. A transition specifies the string to output (or a grammar to generate it) and the next state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "M0uscstwWSBG"
      },
      "outputs": [],
      "source": [
        "default_reber_grammar = [\n",
        "    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n",
        "    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
        "    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
        "    [(\"T\", 3), (\"V\", 5)], # and so on...\n",
        "    [(\"X\", 3), (\"S\", 6)],\n",
        "    [(\"P\", 4), (\"V\", 6)],\n",
        "    [(\"E\", None)]]        # (state 6) =E=>(terminal state)\n",
        "\n",
        "embedded_reber_grammar = [\n",
        "    [(\"B\", 1)],\n",
        "    [(\"T\", 2), (\"P\", 3)],\n",
        "    [(default_reber_grammar, 4)],\n",
        "    [(default_reber_grammar, 5)],\n",
        "    [(\"T\", 6)],\n",
        "    [(\"P\", 6)],\n",
        "    [(\"E\", None)]]\n",
        "\n",
        "def generate_string(grammar):\n",
        "    state = 0\n",
        "    output = []\n",
        "    while state is not None:\n",
        "        index = np.random.randint(len(grammar[state]))\n",
        "        production, state = grammar[state][index]\n",
        "        if isinstance(production, list):\n",
        "            production = generate_string(grammar=production)\n",
        "        output.append(production)\n",
        "    return \"\".join(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zaq_A-9WSBG"
      },
      "source": [
        "Let's generate a few strings based on the default Reber grammar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "r2OirHwdWSBG",
        "outputId": "e7a71085-dbeb-4126-92ed-3fe733e6b707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTXXTTVPXTVPXTTVPSE BPVPSE BTXSE BPVVE BPVVE BTSXSE BPTVPXTTTVVE BPVVE BTXSE BTXXVPSE BPTTTTTTTTVVE BTXSE BPVPSE BTXSE BPTVPSE BTXXTVPSE BPVVE BPVVE BPVVE BPTTVVE BPVVE BPVVE BTXXVVE BTXXVVE BTXXVPXVVE "
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "for _ in range(25):\n",
        "    print(generate_string(default_reber_grammar), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxGcpEcaWSBG"
      },
      "source": [
        "Looks good. Now let's generate a few strings based on the embedded Reber grammar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "HQh9rfs-WSBG",
        "outputId": "ba532d32-cbc5-4c3c-c098-6a4c67cfcd15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTBPTTTVPXTVPXTTVPSETE BPBPTVPSEPE BPBPVVEPE BPBPVPXVVEPE BPBTXXTTTTVVEPE BPBPVPSEPE BPBTXXVPSEPE BPBTSSSSSSSXSEPE BTBPVVETE BPBTXXVVEPE BPBTXXVPSEPE BTBTXXVVETE BPBPVVEPE BPBPVVEPE BPBTSXSEPE BPBPVVEPE BPBPTVPSEPE BPBTXXVVEPE BTBPTVPXVVETE BTBPVVETE BTBTSSSSSSSXXVVETE BPBTSSSXXTTTTVPSEPE BTBPTTVVETE BPBTXXTVVEPE BTBTXSETE "
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "for _ in range(25):\n",
        "    print(generate_string(embedded_reber_grammar), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeR2BUsvWSBG"
      },
      "source": [
        "Okay, now we need a function to generate strings that do not respect the grammar. We could generate a random string, but the task would be a bit too easy, so instead we will generate a string that respects the grammar, and we will corrupt it by changing just one character:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "scpnC52zWSBG"
      },
      "outputs": [],
      "source": [
        "POSSIBLE_CHARS = \"BEPSTVX\"\n",
        "\n",
        "def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n",
        "    good_string = generate_string(grammar)\n",
        "    index = np.random.randint(len(good_string))\n",
        "    good_char = good_string[index]\n",
        "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
        "    return good_string[:index] + bad_char + good_string[index + 1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAQvcxPRWSBG"
      },
      "source": [
        "Let's look at a few corrupted strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "m8hiYnEwWSBG",
        "outputId": "724b20bc-c05a-40c0-f831-095c5bd4266f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTBPTTTPPXTVPXTTVPSETE BPBTXEEPE BPBPTVVVEPE BPBTSSSSXSETE BPTTXSEPE BTBPVPXTTTTTTEVETE BPBTXXSVEPE BSBPTTVPSETE BPBXVVEPE BEBTXSETE BPBPVPSXPE BTBPVVVETE BPBTSXSETE BPBPTTTPTTTTTVPSEPE BTBTXXTTSTVPSETE BBBTXSETE BPBTPXSEPE BPBPVPXTTTTVPXTVPXVPXTTTVVEVE BTBXXXTVPSETE BEBTSSSSSXXVPXTVVETE BTBXTTVVETE BPBTXSTPE BTBTXXTTTVPSBTE BTBTXSETX BTBTSXSSTE "
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "for _ in range(25):\n",
        "    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdk4DQsqWSBG"
      },
      "source": [
        "We cannot feed strings directly to an RNN, so we need to encode them somehow. One option would be to one-hot encode each character. Another option is to use embeddings. Let's go for the second option (but since there are just a handful of characters, one-hot encoding would probably be a good option as well). For embeddings to work, we need to convert each string into a sequence of character IDs. Let's write a function for that, using each character's index in the string of possible characters \"BEPSTVX\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "t-alZxh6WSBH"
      },
      "outputs": [],
      "source": [
        "def string_to_ids(s, chars=POSSIBLE_CHARS):\n",
        "    return [chars.index(c) for c in s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "OwuEvF1FWSBH",
        "outputId": "466b7dc8-7700-4f47-970d-5aa9a7ee9c44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 4, 4, 4, 6, 6, 5, 5, 1, 4, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "string_to_ids(\"BTTTXXVVETE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9BhS2DMWSBH"
      },
      "source": [
        "We can now generate the dataset, with 50% good strings, and 50% bad strings:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for i in range(10):\n",
        "  result += [i]\n",
        "result"
      ],
      "metadata": {
        "id": "1KBc1OXTeNRy",
        "outputId": "bcfe769f-1238-4cac-d903-317004b6fa2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[i for i in range(10)]"
      ],
      "metadata": {
        "id": "eUE5tj_UeVV-",
        "outputId": "c6753508-8558-4343-d077-19a895c31c00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[1,2]+[3,4]"
      ],
      "metadata": {
        "id": "cF99J8v0euHV",
        "outputId": "56d41381-1d79-47ba-f526-ec43dba22bba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([1,2])+np.array([3,4])"
      ],
      "metadata": {
        "id": "iDeIxDo2ejDD",
        "outputId": "1cdb0ec9-1bc2-4909-c03e-15dbf35af8b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "2gV6V6iNWSBH"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(size):\n",
        "    good_strings = [\n",
        "        string_to_ids(generate_string(embedded_reber_grammar))\n",
        "        for _ in range(size // 2)\n",
        "    ]\n",
        "    bad_strings = [\n",
        "        string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n",
        "        for _ in range(size - size // 2)\n",
        "    ]\n",
        "    all_strings = good_strings + bad_strings\n",
        "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
        "    y = np.array([[1.] for _ in range(len(good_strings))] +\n",
        "                 [[0.] for _ in range(len(bad_strings))])\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "tYV7QJa4WSBH"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_train, y_train = generate_dataset(10000)\n",
        "X_valid, y_valid = generate_dataset(2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRFwqK8ZWSBH"
      },
      "source": [
        "Let's take a look at the first training sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "HwRdBYBEWSBH",
        "outputId": "32e24976-b0d6-4afc-b788-da04207702d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(22,), dtype=int32, numpy=\n",
              "array([0, 4, 0, 2, 4, 4, 4, 5, 2, 6, 4, 5, 2, 6, 4, 4, 5, 2, 3, 1, 4, 1],\n",
              "      dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU2O2HXSWSBH"
      },
      "source": [
        "What class does it belong to?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7sPgygeyWSBH",
        "outputId": "14d09da1-f3be-40b6-9c78-c2cfc26108e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K48Cy2BWWSBH"
      },
      "source": [
        "Perfect! We are ready to create the RNN to identify good strings. We build a simple sequence binary classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "9w6CJOQqWSBH",
        "outputId": "1ff78f04-1135-4988-f7fa-a43913be6a4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "313/313 [==============================] - 5s 7ms/step - loss: 0.6934 - accuracy: 0.5173 - val_loss: 0.6864 - val_accuracy: 0.4855\n",
            "Epoch 2/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.6701 - accuracy: 0.5606 - val_loss: 0.6612 - val_accuracy: 0.4500\n",
            "Epoch 3/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.6502 - accuracy: 0.5842 - val_loss: 0.6494 - val_accuracy: 0.5375\n",
            "Epoch 4/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.6370 - accuracy: 0.5949 - val_loss: 0.6545 - val_accuracy: 0.6065\n",
            "Epoch 5/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.6039 - accuracy: 0.6383 - val_loss: 0.5781 - val_accuracy: 0.6855\n",
            "Epoch 6/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.5289 - accuracy: 0.7238 - val_loss: 0.4559 - val_accuracy: 0.8000\n",
            "Epoch 7/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.3136 - accuracy: 0.8772 - val_loss: 0.2151 - val_accuracy: 0.9280\n",
            "Epoch 8/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.2295 - accuracy: 0.9209 - val_loss: 0.1625 - val_accuracy: 0.9525\n",
            "Epoch 9/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.1400 - accuracy: 0.9620 - val_loss: 0.1233 - val_accuracy: 0.9675\n",
            "Epoch 10/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.1419 - accuracy: 0.9578 - val_loss: 0.1210 - val_accuracy: 0.9790\n",
            "Epoch 11/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0932 - accuracy: 0.9771 - val_loss: 0.0708 - val_accuracy: 0.9870\n",
            "Epoch 12/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0438 - accuracy: 0.9900 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0334 - accuracy: 0.9911 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 8.7961e-04 - accuracy: 1.0000 - val_loss: 6.8674e-04 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 6.2996e-04 - accuracy: 1.0000 - val_loss: 5.3469e-04 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 4.9268e-04 - accuracy: 1.0000 - val_loss: 4.1792e-04 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 4.0490e-04 - accuracy: 1.0000 - val_loss: 3.4661e-04 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 3.4254e-04 - accuracy: 1.0000 - val_loss: 3.0063e-04 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 2.9857e-04 - accuracy: 1.0000 - val_loss: 2.6032e-04 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "embedding_size = 5\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
        "    tf.keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS),\n",
        "                              output_dim=embedding_size),\n",
        "    tf.keras.layers.GRU(30),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95,\n",
        "                                    nesterov=True)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJbAHiQvWSBH"
      },
      "source": [
        "Now let's test our RNN on two tricky strings: the first one is bad while the second one is good. They only differ by the second to last character. If the RNN gets this right, it shows that it managed to notice the pattern that the second letter should always be equal to the second to last letter. That requires a fairly long short-term memory (which is the reason why we used a GRU cell)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "IKoyzmhfWSBH",
        "outputId": "0c6aad61-dd11-4d78-931a-9d7de522773e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "\n",
            "Estimated probability that these are Reber strings:\n",
            "BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE: 1.44%\n",
            "BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE: 99.96%\n"
          ]
        }
      ],
      "source": [
        "test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n",
        "                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]\n",
        "X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n",
        "\n",
        "y_proba = model.predict(X_test)\n",
        "print()\n",
        "print(\"Estimated probability that these are Reber strings:\")\n",
        "for index, string in enumerate(test_strings):\n",
        "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aed75dmJWSBH"
      },
      "source": [
        "Ta-da! It worked fine. The RNN found the correct answers with very high confidence. :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdScTTLXWSBH"
      },
      "source": [
        "## 9.\n",
        "_Exercise: Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from \"April 22, 2019\" to \"2019-04-22\")._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP3ehBL8WSBH"
      },
      "source": [
        "Let's start by creating the dataset. We will use random days between 1000-01-01 and 9999-12-31:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZKa9XkcWSBH"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "\n",
        "# cannot use strftime()'s %B format since it depends on the locale\n",
        "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
        "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "\n",
        "def random_dates(n_dates):\n",
        "    min_date = date(1000, 1, 1).toordinal()\n",
        "    max_date = date(9999, 12, 31).toordinal()\n",
        "\n",
        "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
        "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
        "\n",
        "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
        "    y = [dt.isoformat() for dt in dates]\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pJdzmmkWSBH"
      },
      "source": [
        "Here are a few random dates, displayed in both the input format and the target format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYjbZbONWSBH"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_dates = 3\n",
        "x_example, y_example = random_dates(n_dates)\n",
        "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
        "print(\"-\" * 50)\n",
        "for idx in range(n_dates):\n",
        "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQjjtFGtWSBI"
      },
      "source": [
        "Let's get the list of all possible characters in the inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrN-hbdvWSBI"
      },
      "outputs": [],
      "source": [
        "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
        "INPUT_CHARS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkG36PKkWSBI"
      },
      "source": [
        "And here's the list of possible characters in the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvk-KXpuWSBI"
      },
      "outputs": [],
      "source": [
        "OUTPUT_CHARS = \"0123456789-\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9U-UzVfWSBI"
      },
      "source": [
        "Let's write a function to convert a string to a list of character IDs, as we did in the previous exercise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSSG-FwhWSBI"
      },
      "outputs": [],
      "source": [
        "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
        "    return [chars.index(c) for c in date_str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWr8ObAjWSBI"
      },
      "outputs": [],
      "source": [
        "date_str_to_ids(x_example[0], INPUT_CHARS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUQLTTU1WSBI"
      },
      "outputs": [],
      "source": [
        "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWfG-DfaWSBI"
      },
      "outputs": [],
      "source": [
        "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
        "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
        "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
        "    return (X + 1).to_tensor() # using 0 as the padding token ID\n",
        "\n",
        "def create_dataset(n_dates):\n",
        "    x, y = random_dates(n_dates)\n",
        "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mg6VqC1WSBI"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "X_train, Y_train = create_dataset(10000)\n",
        "X_valid, Y_valid = create_dataset(2000)\n",
        "X_test, Y_test = create_dataset(2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwXUTOYVWSBI"
      },
      "outputs": [],
      "source": [
        "Y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0-VtzeoWSBI"
      },
      "source": [
        "### First version: a very basic seq2seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOqs5wLkWSBI"
      },
      "source": [
        "Let's first try the simplest possible model: we feed in the input sequence, which first goes through the encoder (an embedding layer followed by a single LSTM layer), which outputs a vector, then it goes through a decoder (a single LSTM layer, followed by a dense output layer), which outputs a sequence of vectors, each representing the estimated probabilities for all possible output character.\n",
        "\n",
        "Since the decoder expects a sequence as input, we repeat the vector (which is output by the encoder) as many times as the longest possible output sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-Kv0x4FWSBI"
      },
      "outputs": [],
      "source": [
        "embedding_size = 32\n",
        "max_output_length = Y_train.shape[1]\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n",
        "                           output_dim=embedding_size,\n",
        "                           input_shape=[None]),\n",
        "    tf.keras.layers.LSTM(128)\n",
        "])\n",
        "\n",
        "decoder = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "    tf.keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.RepeatVector(max_output_length),\n",
        "    decoder\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Nadam()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, Y_train, epochs=20,\n",
        "                    validation_data=(X_valid, Y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD986UCGWSBI"
      },
      "source": [
        "Looks great, we reach 100% validation accuracy! Let's use the model to make some predictions. We will need to be able to convert a sequence of character IDs to a readable string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYGhf38KWSBI"
      },
      "outputs": [],
      "source": [
        "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
        "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
        "            for sequence in ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QBe8VhFWSBI"
      },
      "source": [
        "Now we can use the model to convert some dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1bo4u5IWSBJ"
      },
      "outputs": [],
      "source": [
        "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5PD-JtUWSBJ"
      },
      "outputs": [],
      "source": [
        "ids = model.predict(X_new).argmax(axis=-1)\n",
        "for date_str in ids_to_date_strs(ids):\n",
        "    print(date_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZrS7FCSWSBJ"
      },
      "source": [
        "Perfect! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu48TcV2WSBJ"
      },
      "source": [
        "However, since the model was only trained on input strings of length 18 (which is the length of the longest date), it does not perform well if we try to use it to make predictions on shorter sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9I8LeErWSBJ"
      },
      "outputs": [],
      "source": [
        "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHjkzVJaWSBJ"
      },
      "outputs": [],
      "source": [
        "ids = model.predict(X_new).argmax(axis=-1)\n",
        "for date_str in ids_to_date_strs(ids):\n",
        "    print(date_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1btVgVaWSBJ"
      },
      "source": [
        "Oops! We need to ensure that we always pass sequences of the same length as during training, using padding if necessary. Let's write a little helper function for that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th290crgWSBJ"
      },
      "outputs": [],
      "source": [
        "max_input_length = X_train.shape[1]\n",
        "\n",
        "def prepare_date_strs_padded(date_strs):\n",
        "    X = prepare_date_strs(date_strs)\n",
        "    if X.shape[1] < max_input_length:\n",
        "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
        "    return X\n",
        "\n",
        "def convert_date_strs(date_strs):\n",
        "    X = prepare_date_strs_padded(date_strs)\n",
        "    ids = model.predict(X).argmax(axis=-1)\n",
        "    return ids_to_date_strs(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxmviRkrWSBJ"
      },
      "outputs": [],
      "source": [
        "convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ospwKDsYWSBJ"
      },
      "source": [
        "Cool! Granted, there are certainly much easier ways to write a date conversion tool (e.g., using regular expressions or even basic string manipulation), but you have to admit that using neural networks is way cooler. ;-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMmocK6pWSBJ"
      },
      "source": [
        "However, real-life sequence-to-sequence problems will usually be harder, so for the sake of completeness, let's build a more powerful model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y48RbmvCWSBJ"
      },
      "source": [
        "### Second version: feeding the shifted targets to the decoder (teacher forcing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLfYIsxkWSBJ"
      },
      "source": [
        "Instead of feeding the decoder a simple repetition of the encoder's output vector, we can feed it the target sequence, shifted by one time step to the right. This way, at each time step the decoder will know what the previous target character was. This should help is tackle more complex sequence-to-sequence problems.\n",
        "\n",
        "Since the first output character of each target sequence has no previous character, we will need a new token to represent the start-of-sequence (sos).\n",
        "\n",
        "During inference, we won't know the target, so what will we feed the decoder? We can just predict one character at a time, starting with an sos token, then feeding the decoder all the characters that were predicted so far (we will look at this in more details later in this notebook).\n",
        "\n",
        "But if the decoder's LSTM expects to get the previous target as input at each step, how shall we pass it it the vector output by the encoder? Well, one option is to ignore the output vector, and instead use the encoder's LSTM state as the initial state of the decoder's LSTM (which requires that encoder's LSTM must have the same number of units as the decoder's LSTM).\n",
        "\n",
        "Now let's create the decoder's inputs (for training, validation and testing). The sos token will be represented using the last possible output character's ID + 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-FL_eZfWSBJ"
      },
      "outputs": [],
      "source": [
        "sos_id = len(OUTPUT_CHARS) + 1\n",
        "\n",
        "def shifted_output_sequences(Y):\n",
        "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
        "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
        "\n",
        "X_train_decoder = shifted_output_sequences(Y_train)\n",
        "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
        "X_test_decoder = shifted_output_sequences(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg6qpkIUWSBJ"
      },
      "source": [
        "Let's take a look at the decoder's training inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUK8dU0iWSBJ"
      },
      "outputs": [],
      "source": [
        "X_train_decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH8eUugLWSBJ"
      },
      "source": [
        "Now let's build the model. It's not a simple sequential model anymore, so let's use the functional API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpMjHggjWSBJ"
      },
      "outputs": [],
      "source": [
        "encoder_embedding_size = 32\n",
        "decoder_embedding_size = 32\n",
        "lstm_units = 128\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "encoder_input = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
        "encoder_embedding = tf.keras.layers.Embedding(\n",
        "    input_dim=len(INPUT_CHARS) + 1,\n",
        "    output_dim=encoder_embedding_size)(encoder_input)\n",
        "_, encoder_state_h, encoder_state_c = tf.keras.layers.LSTM(\n",
        "    lstm_units, return_state=True)(encoder_embedding)\n",
        "encoder_state = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "decoder_input = tf.keras.layers.Input(shape=[None], dtype=tf.int32)\n",
        "decoder_embedding = tf.keras.layers.Embedding(\n",
        "    input_dim=len(OUTPUT_CHARS) + 2,\n",
        "    output_dim=decoder_embedding_size)(decoder_input)\n",
        "decoder_lstm_output = tf.keras.layers.LSTM(lstm_units, return_sequences=True)(\n",
        "    decoder_embedding, initial_state=encoder_state)\n",
        "decoder_output = tf.keras.layers.Dense(len(OUTPUT_CHARS) + 1,\n",
        "                                    activation=\"softmax\")(decoder_lstm_output)\n",
        "\n",
        "model = tf.keras.Model(inputs=[encoder_input, decoder_input],\n",
        "                           outputs=[decoder_output])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Nadam()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n",
        "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Upfx1OUWSBK"
      },
      "source": [
        "This model also reaches 100% validation accuracy, but it does so even faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvZYUHaJWSBK"
      },
      "source": [
        "Let's once again use the model to make some predictions. This time we need to predict characters one by one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU6mdqu3WSBK"
      },
      "outputs": [],
      "source": [
        "sos_id = len(OUTPUT_CHARS) + 1\n",
        "\n",
        "def predict_date_strs(date_strs):\n",
        "    X = prepare_date_strs_padded(date_strs)\n",
        "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
        "    for index in range(max_output_length):\n",
        "        pad_size = max_output_length - Y_pred.shape[1]\n",
        "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
        "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
        "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
        "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
        "    return ids_to_date_strs(Y_pred[:, 1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRBNNj7kWSBK"
      },
      "outputs": [],
      "source": [
        "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L88ZyUQ_WSBK"
      },
      "source": [
        "Works fine! Next, feel free to write a Transformer version. :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGSx6mPbWSBK"
      },
      "source": [
        "## 10.\n",
        "_Exercise: Go through Keras's tutorial for [Natural language image search with a Dual Encoder](https://homl.info/dualtuto). You will learn how to build a model capable of representing both images and text within the same embedding space. This makes it possible to search for images using a text prompt, like in the [CLIP model](https://openai.com/blog/clip/) by OpenAI._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ABm6kOWSBK"
      },
      "source": [
        "Just click the link and follow the instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IeBSkcoWSBK"
      },
      "source": [
        "## 11.\n",
        "_Exercise: Use the Transformers library to download a pretrained language model capable of generating text (e.g., GPT), and try generating more convincing Shakespearean text. You will need to use the model's `generate()` method—see Hugging Face's documentation for more details._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSg9dYAfWSBK"
      },
      "source": [
        "First, let's load a pretrained model. In this example, we will use OpenAI's GPT model, with an additional Language Model on top (just a linear layer with weights tied to the input embeddings). Let's import it and load the pretrained weights (this will download about 445MB of data to `~/.cache/torch/transformers`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEY1hkHXWSBK"
      },
      "outputs": [],
      "source": [
        "from transformers import TFOpenAIGPTLMHeadModel\n",
        "\n",
        "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stv5w06XWSBK"
      },
      "source": [
        "Next we will need a specialized tokenizer for this model. This one will try to use the [spaCy](https://spacy.io/) and [ftfy](https://pypi.org/project/ftfy/) libraries if they are installed, or else it will fall back to BERT's `BasicTokenizer` followed by Byte-Pair Encoding (which should be fine for most use cases)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXL03CkxWSBK"
      },
      "outputs": [],
      "source": [
        "from transformers import OpenAIGPTTokenizer\n",
        "\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2Te3fdnWSBK"
      },
      "source": [
        "Now let's use the tokenizer to tokenize and encode the prompt text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa_QVXiKWSBK"
      },
      "outputs": [],
      "source": [
        "tokenizer(\"hello everyone\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKihE_ESWSBK"
      },
      "outputs": [],
      "source": [
        "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
        "encoded_prompt = tokenizer.encode(prompt_text,\n",
        "                                  add_special_tokens=False,\n",
        "                                  return_tensors=\"tf\")\n",
        "encoded_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaiRG90eWSBK"
      },
      "source": [
        "Easy! Next, let's use the model to generate text after the prompt. We will generate 5 different sentences, each starting with the prompt text, followed by 40 additional tokens. For an explanation of what all the hyperparameters do, make sure to check out this great [blog post](https://huggingface.co/blog/how-to-generate) by Patrick von Platen (from Hugging Face). You can play around with the hyperparameters to try to obtain better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7eSWa_JWSBK"
      },
      "outputs": [],
      "source": [
        "num_sequences = 5\n",
        "length = 40\n",
        "\n",
        "generated_sequences = model.generate(\n",
        "    input_ids=encoded_prompt,\n",
        "    do_sample=True,\n",
        "    max_length=length + len(encoded_prompt[0]),\n",
        "    temperature=1.0,\n",
        "    top_k=0,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.0,\n",
        "    num_return_sequences=num_sequences,\n",
        ")\n",
        "\n",
        "generated_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aULS-04sWSBK"
      },
      "source": [
        "Now let's decode the generated sequences and print them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rlo14QnKWSBK"
      },
      "outputs": [],
      "source": [
        "for sequence in generated_sequences:\n",
        "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
        "    print(text)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttr8-HLFWSBK"
      },
      "source": [
        "You can try more recent (and larger) models, such as GPT-2, CTRL, Transformer-XL or XLNet, which are all available as pretrained models in the transformers library, including variants with Language Models on top. The preprocessing steps vary slightly between models, so make sure to check out this [generation example](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py) from the transformers documentation (this example uses PyTorch, but it will work with very little tweaks, such as adding `TF` at the beginning of the model class name, removing the `.to()` method calls, and using `return_tensors=\"tf\"` instead of `\"pt\"`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5TOYO3HWSBL"
      },
      "source": [
        "Hope you enjoyed this chapter! :)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}